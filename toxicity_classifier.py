# -*- coding: utf-8 -*-
"""Toxicity_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iMei6d-rYmrVbrNi502VXkPakIqgkbUK
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

data = pd.read_csv("FinalBalancedDataset.csv")

data.info()

data.head(5)

data = data.drop("Unnamed: 0", axis=1)

data.head(5)

data['Toxicity'].value_counts()

import nltk
nltk.download('punkt')
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
from nltk import WordNetLemmatizer
from nltk import pos_tag, word_tokenize
from nltk.corpus import stopwords as nltk_stopwords
from nltk.corpus import wordnet

"""## Text pre-processing"""

wordnet_lemmatizer = WordNetLemmatizer()

import re
import nltk

def prepare_text(text):
    def get_wordnet_pos(treebank_tag):
        if treebank_tag.startswith('J'):
            return wordnet.ADJ
        elif treebank_tag.startswith('V'):
            return wordnet.VERB
        elif treebank_tag.startswith('N'):
            return wordnet.NOUN
        elif treebank_tag.startswith('R'):
            return wordnet.ADV
        else:
            return wordnet.NOUN
    text = re.sub(r'[^a-zA-Z\']', ' ', text)
    text = text.split()
    text = ' '.join(text)
    text = word_tokenize(text)
    text = pos_tag(text)
    lemma = []
    for i in text: lemma.append(wordnet_lemmatizer.lemmatize(i[0], pos = get_wordnet_pos(i[1])))
    lemma = ' '.join(lemma)
    return lemma

data['clean_tweets'] = data['tweet'].apply(lambda x: prepare_text(x))

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')

data.head(5)

"""## Tfidf for features"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
import pickle

stopwords = list(nltk_stopwords.words('english'))

corpus = data['clean_tweets'].values.astype('U')

count_tf_idf = TfidfVectorizer(stop_words=stopwords)

tf_idf = count_tf_idf.fit_transform(corpus)

tf_idf_train, tf_idf_test, target_train, target_test = train_test_split(
    tf_idf, data['Toxicity'], test_size = 0.8, random_state= 42, shuffle=True
)

"""

## Binary Classification Model"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score
models = {
    'Naive Bayes': MultinomialNB(),
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(probability=True)  # enable probability estimates for ROC curve
}

results = {}
for model_name, model in models.items():
    model.fit(tf_idf_train, target_train)
    y_pred_proba = model.predict_proba(tf_idf_test)[::, 1]

    roc_auc = roc_auc_score(target_test, y_pred_proba)

    y_pred = model.predict(tf_idf_test)
    accuracy = accuracy_score(target_test, y_pred)


    results[model_name] = {'ROC AUC': roc_auc, 'Accuracy': accuracy}

    print(f"{model_name}: ROC AUC = {roc_auc:.4f}, Accuracy = {accuracy:.4f}")

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score, accuracy_score
import matplotlib.pyplot as plt



models = {
    'Naive Bayes': MultinomialNB(),
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(probability=True)
}

results = {}
for model_name, model in models.items():
    model.fit(tf_idf_train, target_train)
    y_pred_proba = model.predict_proba(tf_idf_test)[:, 1]

    roc_auc = roc_auc_score(target_test, y_pred_proba)
    y_pred = model.predict(tf_idf_test)
    accuracy = accuracy_score(target_test, y_pred)

    results[model_name] = {'ROC AUC': roc_auc, 'Accuracy': accuracy}

    print(f"{model_name}: ROC AUC = {roc_auc:.4f}, Accuracy = {accuracy:.4f}")


fig, ax = plt.subplots(1, 2, figsize=(12, 6))

model_names = list(results.keys())
roc_auc_values = [results[model]['ROC AUC'] for model in model_names]

ax[0].bar(model_names, roc_auc_values, color='skyblue')
ax[0].set_title('ROC AUC Comparison')
ax[0].set_ylabel('ROC AUC')
ax[0].set_xlabel('Models')
ax[0].set_ylim(0, 1)
accuracy_values = [results[model]['Accuracy'] for model in model_names]

ax[1].bar(model_names, accuracy_values, color='lightgreen')
ax[1].set_title('Accuracy Comparison')
ax[1].set_ylabel('Accuracy')
ax[1].set_xlabel('Models')
ax[1].set_ylim(0, 1)

plt.tight_layout()
plt.show()

import pandas as pd
model=SVC(probability=True)
model.fit(tf_idf_train, target_train)
test_data = pd.read_csv('reddit_comments_raw.csv')

comments = test_data['comment'].values.astype('U')

tf_idf_test = count_tf_idf.transform(comments)

test_predictions = model.predict(tf_idf_test)

test_data['toxicity_label'] = test_predictions  # Add class predictions (0 or 1)

test_data.to_csv('test_data_with_toxicity_label.csv', index=False)

print("Toxicity labels added and saved to 'test_data_with_toxicity_label.csv'")

"""##Bert Implementation"""

pip install transformers torch

from transformers import BertTokenizer, BertForSequenceClassification
from transformers import AdamW
from torch.utils.data import Dataset, DataLoader
import torch

class TweetDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
MAX_LEN = 128
BATCH_SIZE = 16

train_dataset = TweetDataset(
    texts=data['clean_tweets'].tolist(),
    labels=data['Toxicity'].tolist(),
    tokenizer=tokenizer,
    max_len=MAX_LEN
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model = model.to('cuda' if torch.cuda.is_available() else 'cpu')

optimizer = AdamW(model.parameters(), lr=2e-5)

model.train()
for epoch in range(3):
    for batch in train_loader:
        input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')
        attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')
        labels = batch['labels'].to('cuda' if torch.cuda.is_available() else 'cpu')

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

test_data = pd.read_csv('reddit_comments_raw.csv')
comments = test_data['comment'].values.astype('U')

test_dataset = TweetDataset(
    texts=comments,
    labels=[0]*len(comments),
    tokenizer=tokenizer,
    max_len=MAX_LEN
)

test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

model.eval()

predictions = []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')
        attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        _, preds = torch.max(outputs.logits, dim=1)
        predictions.extend(preds.cpu().numpy())

test_data['toxicity_label'] = predictions
test_data.to_csv('test_data_with_toxicity_label.csv', index=False)
print("Toxicity labels added and saved to 'test_data_with_toxicity_label.csv'")

from sklearn.metrics import roc_auc_score, accuracy_score

roc_auc = roc_auc_score(target_test, predictions)
accuracy = accuracy_score(target_test, predictions)

print(f"BERT Model: ROC AUC = {roc_auc:.4f}, Accuracy = {accuracy:.4f}")