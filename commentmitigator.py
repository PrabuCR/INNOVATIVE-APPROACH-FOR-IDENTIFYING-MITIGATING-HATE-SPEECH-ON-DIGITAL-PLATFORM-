# -*- coding: utf-8 -*-
"""CommentMitigator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TKEPnoNDh6kWm3l0LetdgjtBKbCcw9FY

**Installing the Libraries :**
*   Transformers
*   Torch
*   Praw
*   Pandas
*   Better-Profanity
"""

!pip install transformers torch praw pandas
!pip install better-profanity

"""**Realtime Data Collection** [PRAW/BEATUIFUL SOUP]

**Using PRAW** (REDDIT only)
"""

import praw
import pandas as pd
import uuid

# Initialize the Reddit API client
reddit = praw.Reddit(
    client_id="m8zjJ9dPeiK3tPjBsggGFA",
    client_secret="mm5uyYA2CT1TezA9wueRHMN1Wmic9Q",
    user_agent="u/CommentMitigator",
)

# Choose a post to scrape comments from
post_url = "https://www.reddit.com/r/pics/comments/1g4426i/mugshot_of_dilawar_an_afghan_taxi_driver_who_was/"
submission = reddit.submission(url=post_url)

# Prepare a list to store comment data
data = []

# Retrieve and flatten the comment tree
submission.comments.replace_more(limit=0)

# Extract user and comment information
for comment in submission.comments:
    user = f"u/{comment.author.name}" if comment.author else "u/Deleted"
    body = comment.body
    comment_id = str(uuid.uuid4())  # Generate a unique ID

    # Append user, comment, and ID data into the list
    data.append([comment_id, user, body])

# Save the collected data into a DataFrame
df = pd.DataFrame(data, columns=["id", "user", "comment"])
df.to_csv("reddit_comments_raw.csv", index=False)

print("Raw comments saved to 'reddit_comments_raw.csv'!")

"""**Using Youtube-API**"""

from googleapiclient.discovery import build
import pandas as pd
import uuid

API_KEY = "YOUR_YOUTUBE_API_KEY"  # Replace with your actual API key
youtube = build("youtube", "v3", developerKey=API_KEY)


def get_youtube_comments(video_id, max_results=100):
    comments = []
    request = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=max_results,
        textFormat="plainText"
    )

    while request:
        response = request.execute()

        # Process each comment
        for item in response.get("items", []):
            comment = item["snippet"]["topLevelComment"]["snippet"]
            comment_id = str(uuid.uuid4())  # Generate a unique ID
            user = comment.get("authorDisplayName", "N/A")
            text = comment.get("textDisplay", "")

            # Store data in the list
            comments.append([comment_id, user, text])

        # Check if there are more comments to fetch
        request = youtube.commentThreads().list_next(request, response)

    return comments

video_id = "dQw4w9WgXcQ"

# Get comments from the video
data = get_youtube_comments(video_id)

# Save the collected data into a DataFrame
df = pd.DataFrame(data, columns=["id", "user", "comment"])
df.to_csv("youtube_comments.csv", index=False)

print("YouTube comments saved to 'youtube_comments.csv'!")

"""Twitter V2 (Tweepy)"""

import tweepy
import pandas as pd
import uuid

# X API (Twitter API) credentials
BEARER_TOKEN = "YOUR_BEARER_TOKEN"  # Replace with your Bearer Token

# Set up the X (Twitter) client using Tweepy
client = tweepy.Client(bearer_token=BEARER_TOKEN)

# Function to get recent posts based on a query
def get_posts_from_x(query, max_results=100):
    posts = []

    # Search recent posts with the given query
    response = client.search_recent_tweets(
        query=query, tweet_fields=["author_id"], max_results=max_results
    )

    # Extract relevant data (id, user_id, and text)
    for post in response.data:
        post_id = str(uuid.uuid4())  # Generate a unique ID
        user_id = post.author_id
        text = post.text
        posts.append([post_id, user_id, text])

    return posts

# Query to search (e.g., "AI trends")
query = "AI trends"

data = get_posts_from_x(query)

df = pd.DataFrame(data, columns=["id", "user", "comment"])
df.to_csv("x_posts.csv", index=False)

print("Posts saved to 'x_posts.csv'!")

"""**Using Beautiful Soup**"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import uuid

# URL of the Reddit post to scrape
url = "https://www.reddit.com/r/pics/comments/1g4426i/mugshot_of_dilawar_an_afghan_taxi_driver_who_was/"

# Send a request and parse the page content
headers = {"User-Agent": "Mozilla/5.0"}
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, "html.parser")

# Extract user comments from the HTML
comments = soup.find_all("div", class_="md")  # Adjust the tag/class based on Reddit structure

# Prepare a list to store comment data
data = []

for comment in comments:
    body = comment.get_text(strip=True)
    comment_id = str(uuid.uuid4())  # Generate a unique ID
    user = "N/A"  # Scraping may not always retrieve user info

    data.append([comment_id, user, body])

# Save the collected data into a DataFrame
df = pd.DataFrame(data, columns=["id", "user", "comment"])
df.to_csv("reddit_comments_raw.csv", index=False)

print("Raw comments saved to 'reddit_comments_raw.csv'!")

"""**Sentiment Analysis on Collected Data**"""

import pandas as pd
from transformers import pipeline
from better_profanity import profanity

# Load the sentiment analysis pipeline
sentiment_analyzer = pipeline(
    "sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english", truncation=True
)

# Load the raw comments CSV
df = pd.read_csv("reddit_comments_raw.csv")

# Function to split long comments into smaller chunks (max 512 tokens each)
def split_into_chunks(text, max_length=512):
    words = text.split()
    for i in range(0, len(words), max_length):
        yield " ".join(words[i:i + max_length])

# Analyze sentiment and check for profanity
results = []

for index, row in df.iterrows():
    comment_id = row["id"]
    user = row["user"]
    body = row["comment"]

    # Handle long comments by splitting into chunks
    sentiments = [sentiment_analyzer(chunk)[0] for chunk in split_into_chunks(body)]

    # Aggregate sentiment results
    positive_count = sum(1 for s in sentiments if s['label'] == 'POSITIVE')
    negative_count = sum(1 for s in sentiments if s['label'] == 'NEGATIVE')

    # Determine if the comment contains obscene language
    obscene_flag = profanity.contains_profanity(body)

    # Assign only one column to 1 based on priority: Obscene > Negative > Positive
    positive = 0
    negative = 0
    obscene = 0

    if obscene_flag:
        obscene = 1
    elif negative_count > positive_count:
        negative = 1
    else:
        positive = 1

    # Store the results
    results.append([comment_id, user, body, positive, negative, obscene])

# Save the results to a new DataFrame
df_results = pd.DataFrame(results, columns=["id", "user", "comment", "positive", "negative", "obscene"])

# Save to CSV
df_results.to_csv("reddit_comments_analyzed.csv", index=False)

print("Sentiment analysis results saved to 'reddit_comments_analyzed.csv'!")